# llm-context-lambda-
Serverless LLM inference pipeline on AWS using Lambda, S3, and SageMaker, demonstrating lightweight RAG via prompt-based context injection and cost-aware deployment.
